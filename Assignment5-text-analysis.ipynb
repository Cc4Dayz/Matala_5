{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Text Analysis\n",
    "An explanation this assignment could be found in the .pdf explanation document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Materials to review for this assignment\n",
    "<h4>From Moodle:</h4> \n",
    "<h5><u>Review the notebooks regarding the following python topics</u>:</h5>\n",
    "<div class=\"alert alert-info\">\n",
    "&#x2714; <b>Working with strings</b> (tutorial notebook)<br/>\n",
    "&#x2714; <b>Text Analysis</b> (tutorial notebook)<br/>\n",
    "&#x2714; <b>Hebrew text analysis tools (tokenizer, wordnet)</b> (moodle example)<br/>\n",
    "&#x2714; <b>(brief review) All previous notebooks</b><br/>\n",
    "</div> \n",
    "<h5><u>Review the presentations regarding the following topics</u>:</h5>\n",
    "<div class=\"alert alert-info\">\n",
    "&#x2714; <b>Text Analysis</b> (lecture presentation)<br/>\n",
    "&#x2714; <b>(brief review) All other presentations</b><br/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preceding Step - import modules (packages)\n",
    "This step is necessary in order to use external modules (packages). <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# ------------- visualizations:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics, pipeline, model_selection, feature_extraction \n",
    "from sklearn import naive_bayes, linear_model, svm, neural_network, neighbors, tree\n",
    "from sklearn import decomposition, cluster\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2 ,mutual_info_classif\n",
    "from sklearn.model_selection import  cross_validate ,RepeatedKFold , RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import StackingClassifier , BaggingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC \n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "# ----------------- output and visualizations: \n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "# show several prints in one cell. This will allow us to condence every trick in one cell.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "# ---------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text analysis and String manipulation imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# --------- Text analysis and Hebrew text analysis imports:\n",
    "# vectorizers:\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# regular expressions:\n",
    "import re\n",
    "# --------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Hebrew text analysis - WordNet (for Hebrew)\n",
    "Note: the WordNet is not a must"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Only if you didn't install Wordnet (for Hebrew) use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word net installation:\n",
    "\n",
    "# unmark if you want to use and need to install\n",
    "# !pip install wn\n",
    "# !python -m wn download omw-he:1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word net import:\n",
    "\n",
    "# unmark if you want to use:\n",
    "# import wn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Hebrew text analysis - hebrew_tokenizer (Tokenizer for Hebrew)\n",
    "Note: the hebrew_tokenizer is not a must"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Only if you didn't install hebrew_tokenizer use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hebrew tokenizer installation:\n",
    "\n",
    "# unmark if you want to use and need to install:\n",
    "# !pip install hebrew_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hebrew tokenizer import:\n",
    "\n",
    "# unmark if you want to use:\n",
    "# import hebrew_tokenizer as ht"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading input files\n",
    "Reading input files for train annotated corpus (raw text data) corpus and for the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'annotated_corpus_for_train.csv'\n",
    "test_filename  = 'corpus_for_test.csv'\n",
    "df_train = pd.read_csv(train_filename, index_col=None, encoding='utf-8')\n",
    "df_test  = pd.read_csv(test_filename, index_col=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>כשחבר הזמין אותי לחול, לא באמת חשבתי שזה יקרה,...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>לפני שהתגייסתי לצבא עשיתי כל מני מיונים ליחידו...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>מאז שהתחילו הלימודים חלומו של כל סטודנט זה הפנ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>כשהייתי ילד, מטוסים היה הדבר שהכי ריתק אותי. ב...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‏הייתי מדריכה בכפר נוער ומתאם הכפר היינו צריכי...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>לפני כ3 חודשים טסתי לרומא למשך שבוע. טסתי במטו...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>אני כבר שנתיים נשוי והשנה אני ואישתי סוף סוף י...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>השנה התחלנו שיפוץ בדירה שלנו בתל אביב. הדירה ה...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story gender\n",
       "0  כשחבר הזמין אותי לחול, לא באמת חשבתי שזה יקרה,...      m\n",
       "1  לפני שהתגייסתי לצבא עשיתי כל מני מיונים ליחידו...      m\n",
       "2  מאז שהתחילו הלימודים חלומו של כל סטודנט זה הפנ...      f\n",
       "3  כשהייתי ילד, מטוסים היה הדבר שהכי ריתק אותי. ב...      m\n",
       "4  ‏הייתי מדריכה בכפר נוער ומתאם הכפר היינו צריכי...      f\n",
       "5  לפני כ3 חודשים טסתי לרומא למשך שבוע. טסתי במטו...      f\n",
       "6  אני כבר שנתיים נשוי והשנה אני ואישתי סוף סוף י...      m\n",
       "7  השנה התחלנו שיפוץ בדירה שלנו בתל אביב. הדירה ה...      f"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(753, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(8)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_example_id</th>\n",
       "      <th>story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>כל קיץ אני והמשפחה נוסעים לארצות הברית לוס אנג...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>הגעתי לשירות המדינה אחרי שנתיים כפעיל בתנועת \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>אחת האהבות הגדולות שלי אלו הכלבים שלי ושל אישת...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_example_id                                              story\n",
       "0                0  כל קיץ אני והמשפחה נוסעים לארצות הברית לוס אנג...\n",
       "1                1  הגעתי לשירות המדינה אחרי שנתיים כפעיל בתנועת \"...\n",
       "2                2  אחת האהבות הגדולות שלי אלו הכלבים שלי ושל אישת..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(323, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(3)\n",
    "df_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your implementation:\n",
    "Write your code solution in the following code-cells"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student: Ofek Atun\n",
    "\n",
    "ID: 316063015\n",
    "\n",
    "Email: Ofek.Atun14@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pattern matching librarry \n",
    "import re\n",
    "\n",
    "# This function gets a dataframe with Row named \"story\" and remove un needed data.\n",
    "def clean_text_df(dataframe):\n",
    "    for index, row in dataframe.iterrows():\n",
    "        story = row[\"story\"]\n",
    "        # Remove numbers\n",
    "        story = re.sub(r'\\d+', '', story)\n",
    "        # Remove non-alphanumeric characters\n",
    "        story = re.sub(r'[^\\w\\s]', '', story)\n",
    "        # Remove extra spaces\n",
    "        story = re.sub(r'\\s+', ' ', story)\n",
    "        # Strip leading and trailing spaces\n",
    "        story = story.strip()\n",
    "        # Update the cleaned story in the DataFrame\n",
    "        dataframe.at[index, \"story\"] = story\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "df_train = clean_text_df(df_train)\n",
    "df_test = clean_text_df(df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data to Train and Test for out machine learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_data(dataframe):\n",
    "    # Split the dataframe into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataframe[\"story\"], dataframe[\"gender\"], test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Split the training dataframe into train and test sets\n",
    "X_train, X_test, y_train, y_test = split_train_test_data(df_train)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vectorization for Text Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_vectors(X_train, X_test, ngram_range=(1, 1), min_document_frequency=5):\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=min_document_frequency, ngram_range=ngram_range)\n",
    "    \n",
    "    # Convert the training set into TF-IDF \n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Convert the test set into TF-IDF \n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    return X_train_tfidf, X_test_tfidf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_features(X_train, X_test, y_train, k=800):\n",
    "    feature_selector = SelectKBest(mutual_info_classif, k=k)\n",
    "    \n",
    "    # Fit the feature selector using the training set and target variable\n",
    "    feature_selector.fit(X_train, y_train)\n",
    "    \n",
    "    # Transform the training set to retain only the selected features\n",
    "    X_train_selected = feature_selector.transform(X_train)\n",
    "    \n",
    "    # Transform the test set to retain only the selected features\n",
    "    X_test_selected = feature_selector.transform(X_test)\n",
    "    \n",
    "    return X_train_selected, X_test_selected\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling using Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_MinMaxScale(X_train_selected, X_test_selected):\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Scale the training set using MinMaxScaler\n",
    "    X_train_scaled = scaler.fit_transform(X_train_selected.toarray())\n",
    "    \n",
    "    # Scale the test set using MinMaxScaler\n",
    "    X_test_scaled = scaler.transform(X_test_selected.toarray())\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Best Hyper-Parameters using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_HyParams_grid_search(model, parameters, X_train, y_train):\n",
    "    # Perform grid search to find the best parameters\n",
    "    grid_search = GridSearchCV(model, parameters, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Return the best parameters found\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best hyper-parameters with all the fallowing Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LinearSVC:{'C': 0.1, 'dual': False, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "linear_svc_params = [{'C': [0.01, 0.1, 1, 10, 100],  'penalty': [None, 'l1', 'l2'], 'dual': [False]}]\n",
    "best_params_linear_svc = find_best_HyParams_grid_search(LinearSVC(), linear_svc_params, X_train_scaled, y_train)\n",
    "print(\"Best parameters for LinearSVC:\" + str(best_params_linear_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Perceptron:{'alpha': 0.0001, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "perceptron_params = [{'alpha': [0.0001, 0.05], 'penalty': [None, 'l2', 'l1', 'elasticnet']}]\n",
    "best_params_perceptron = find_best_HyParams_grid_search(Perceptron(), perceptron_params, X_train_scaled, y_train)\n",
    "print(\"Best parameters for Perceptron:\" + str(best_params_perceptron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for MultinomialNB:{'alpha': 1, 'class_prior': [0.3, 0.7], 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "multinomialNB_params = [{ 'alpha': [0.01, 0.1, 0.5, 1],  'fit_prior': [True, False],  'class_prior': [None, [0.5, 0.5], [0.3, 0.7]]}]\n",
    "best_params_multinomialNB = find_best_HyParams_grid_search(MultinomialNB(), multinomialNB_params, X_train_scaled, y_train)\n",
    "print(\"Best parameters for MultinomialNB:\" + str(best_params_multinomialNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for MLPClassifier:{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "#takes alot of time to run, run if needed\n",
    "mlp_params = [\n",
    "    {\n",
    "        'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "]\n",
    "\n",
    "best_params_mlp = find_best_HyParams_grid_search(MLPClassifier(), mlp_params, X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters for MLPClassifier:\" + str(best_params_mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SGDClassifier:{'alpha': 0.05, 'loss': 'modified_huber', 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sgd_parmas = [{'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "               'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "               'alpha': [0.0001, 0.05]}]\n",
    "best_params_sgd = find_best_HyParams_grid_search(SGDClassifier(), sgd_parmas, X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters for SGDClassifier:\" + str(best_params_sgd))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model`s Initialization with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearSVC(**best_params_linear_svc)  \n",
    "perceptron_model = Perceptron(**best_params_perceptron)  \n",
    "multinomialNB_model = MultinomialNB(**best_params_multinomialNB)  \n",
    "sgd_model = SGDClassifier(**best_params_sgd) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run only if used MLP\n",
    "mlp_model = MLPClassifier(**best_params_mlp) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is Training, Predicting, and Evaluating a Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    # Train the model using the training data\n",
    "    model_trained = model.fit(X_train, y_train)\n",
    "    \n",
    "    # Define the cross-validation strategy\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    \n",
    "    # Define the scoring metric (F1 score with micro averaging)\n",
    "    f1_scorer = make_scorer(f1_score, average='micro')\n",
    "    \n",
    "    # Perform cross-validation and calculate F1 scores\n",
    "    scores = cross_val_score(model_trained, X_test, y_test, scoring=f1_scorer, cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # Print the F1 scores obtained during cross-validation\n",
    "    print(scores)\n",
    "    \n",
    "    # Predict the target variable on the test data\n",
    "    y_pred = model_trained.predict(X_test)\n",
    "    \n",
    "    # Print the predicted labels\n",
    "    print(y_pred)\n",
    "    \n",
    "    # Calculate the F1 scores for each class (male and female)\n",
    "    f1_male = f1_score(y_test, y_pred, pos_label=\"m\")\n",
    "    f1_female = f1_score(y_test, y_pred, pos_label=\"f\")\n",
    "    \n",
    "    # Calculate the average F1 score\n",
    "    f1_average = (f1_male + f1_female) / 2\n",
    "    \n",
    "    # Return the trained model and the average F1 score\n",
    "    return model_trained, f1_average\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, Predicting, and Evaluating our Model`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74193548 0.56666667 0.83333333 0.83333333 0.83333333 0.87096774\n",
      " 0.86666667 0.8        0.7        0.7        0.74193548 0.63333333\n",
      " 0.7        0.93333333 0.8       ]\n",
      "['m' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'm' 'm' 'm' 'f' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'f' 'm' 'm' 'm' 'm' 'm'\n",
      " 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm'\n",
      " 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm']\n",
      "f1_score Linear_svc: 0.6864303616183316\n",
      "[0.74193548 0.6        0.86666667 0.73333333 0.83333333 0.83870968\n",
      " 0.83333333 0.76666667 0.76666667 0.7        0.74193548 0.66666667\n",
      " 0.76666667 0.9        0.76666667]\n",
      "['m' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'f' 'f' 'm' 'f' 'm' 'm' 'm' 'f' 'm' 'm'\n",
      " 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'f' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'f' 'm' 'm' 'm' 'f'\n",
      " 'm' 'm' 'm' 'm' 'f' 'm' 'f' 'f' 'f' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'f'\n",
      " 'f' 'm' 'f' 'f' 'f' 'f' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f'\n",
      " 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm']\n",
      "f1_score perceptron: 0.7136083451872925\n",
      "[0.77419355 0.66666667 0.73333333 0.8        0.83333333 0.77419355\n",
      " 0.83333333 0.8        0.73333333 0.7        0.80645161 0.56666667\n",
      " 0.66666667 0.93333333 0.76666667]\n",
      "['m' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'f' 'm' 'f' 'm' 'm' 'm' 'f' 'm' 'm'\n",
      " 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm'\n",
      " 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm']\n",
      "f1_score MultinomialNB: 0.7080558539205155\n",
      "[0.70967742 0.6        0.86666667 0.86666667 0.8        0.90322581\n",
      " 0.8        0.76666667 0.7        0.7        0.80645161 0.66666667\n",
      " 0.7        0.9        0.8       ]\n",
      "['m' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'f' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm'\n",
      " 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm']\n",
      "f1_score SGD: 0.6165079365079366\n"
     ]
    }
   ],
   "source": [
    "linear_trained, f1_average_linear = fit_predict_evaluate(linear_model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "print(\"f1_score Linear_svc: \" + str(f1_average_linear))\n",
    "\n",
    "perceptron_trained, f1_average_perceptron = fit_predict_evaluate(perceptron_model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "print(\"f1_score perceptron: \" + str(f1_average_perceptron))\n",
    "\n",
    "multinomialNB_trained, f1_average_multinomialNB = fit_predict_evaluate(multinomialNB_model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "print(\"f1_score MultinomialNB: \" + str(f1_average_multinomialNB))\n",
    "\n",
    "sgd_trained, f1_average_sgd = fit_predict_evaluate(sgd_model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "print(\"f1_score SGD: \" + str(f1_average_sgd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70967742 0.56666667 0.83333333 0.86666667 0.8        0.77419355\n",
      " 0.83333333 0.76666667 0.7        0.7        0.80645161 0.6\n",
      " 0.7        0.93333333 0.8       ]\n",
      "['m' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'm' 'm' 'f' 'f' 'm' 'm'\n",
      " 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'f' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'm'\n",
      " 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'f' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm'\n",
      " 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'f' 'm' 'm' 'm'\n",
      " 'm' 'f' 'm' 'm' 'm' 'm' 'm']\n",
      "f1_score MLP:0.6880165289256198\n"
     ]
    }
   ],
   "source": [
    "#run only if used MLP\n",
    "mlp_trained, f1_average_mlp = fit_predict_evaluate(mlp_model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "print(\"f1_score MLP:\" + str(f1_average_mlp))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create TF-IDF vectors from text data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tfidf_vectors(X, ngram_range=(1, 1), min_df=5):\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df, ngram_range=ngram_range)\n",
    "    \n",
    "    # Convert the input data X into TF-IDF vectors\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "    \n",
    "    # Return the TF-IDF vectors\n",
    "    return X_tfidf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to select the best features using Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_best_features_test(X, k=800):\n",
    "    selector = SelectKBest(mutual_info_classif, k=k)\n",
    "    \n",
    "    # Perform feature selection using Mutual Information\n",
    "    # The second argument is a placeholder array since the target variable is not used in this case\n",
    "    X_selected = selector.fit_transform(X, np.zeros(X.shape[0]))\n",
    "    \n",
    "    # Return the selected features\n",
    "    return X_selected\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to perform Min-Max scaling on feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_MinMaxScale_test(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Perform Min-Max scaling on the feature vectors\n",
    "    # The `toarray()` method is used to convert sparse matrices to dense arrays\n",
    "    X_scaled = scaler.fit_transform(X.toarray())\n",
    "    \n",
    "    # Return the scaled feature vectors\n",
    "    return X_scaled\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming and predicting on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1000 features, but SGDClassifier is expecting 800 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m X_df_test \u001b[39m=\u001b[39m perform_MinMaxScale_test(X_df_test)  \u001b[39m# Performing min-max scaling on X_df_test\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Predicting the categories on df_test using the trained model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m y_prediction_test \u001b[39m=\u001b[39m sgd_trained\u001b[39m.\u001b[39;49mpredict(X_df_test)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Extracting the text example IDs from df_test\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df_text_example \u001b[39m=\u001b[39m df_test\u001b[39m.\u001b[39mtest_example_id\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[39mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 419\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[0;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    421\u001b[0m     indices \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    397\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    398\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 400\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    401\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[0;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39mreshape(scores, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 588\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    590\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1000 features, but SGDClassifier is expecting 800 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "X_df_test = df_test[\"story\"]  # Extracting the stories from df_test\n",
    "X_df_test = create_tfidf_vectors(X_df_test)  # Applying TF-IDF vectorization to X_df_test\n",
    "X_df_test = select_best_features_test(X_df_test)  # Selecting the k best features from X_df_test\n",
    "X_df_test = perform_MinMaxScale_test(X_df_test)  # Performing min-max scaling on X_df_test\n",
    "\n",
    "# Predicting the categories on df_test using the trained model\n",
    "y_prediction_test = sgd_trained.predict(X_df_test)\n",
    "\n",
    "# Extracting the text example IDs from df_test\n",
    "df_text_example = df_test.test_example_id\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output to csv (optional)\n",
    "After you're done save your output to the 'classification_results.csv' csv file.<br/>\n",
    "We assume that the dataframe with your results contain the following columns:\n",
    "* column 1 (left column): 'test_example_id'  - the same id associated to each of the test stories to be predicted.\n",
    "* column 2 (right column): 'predicted_category' - the predicted gender value for each of the associated story. \n",
    "\n",
    "Assuming your predicted values are in the `df_predicted` dataframe, you should save you're results as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the predicted categories and their corresponding text example IDs\n",
    "df_predicted = pd.DataFrame({\n",
    "    \"test_example_id\": df_text_example.tolist(),\n",
    "    \"predicted_category\": y_prediction_test.tolist()\n",
    "})  \n",
    "\n",
    "df_predicted.to_csv('classification_results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
